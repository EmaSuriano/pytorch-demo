{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaSuriano/pytorch-demo/blob/main/entrega/Proyecto%20pr%C3%A1ctico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "\n",
        "*   Alumno 1: José Fernández López\n",
        "*   Alumno 2: Julio Emanuel Suriano Bryk\n",
        "*   Alumno 3: Wilmer Patricio Pujos Castro\n",
        "*   Alumno 4: Fernando Javier Vera Pérez\n",
        "\n",
        "Nombre de grupo: Grupo_gc_7"
      ],
      "metadata": {
        "id": "3bjb7ZUaXHcs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_YDFwZ-JscI"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6n7MIefJ21i",
        "outputId": "a09047a3-6061-4943-ad22-21f2b28e0fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos en el directorio: \n",
            "['.ipynb_checkpoints', 'checkpoint', 'dqn_SpaceInvaders-v0_log.json', 'models_trained', 'notebook_practicas', 'Proyecto_práctico.ipynb', 'Proyecto_práctico2.ipynb']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbVRjvHCJ8UF",
        "outputId": "b974b40c-a27c-4118-95c3-8c6db527cdf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym==0.17.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from gym==0.17.3) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from gym==0.17.3) (1.24.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from gym==0.17.3) (1.6.0)\n",
            "Requirement already satisfied: future in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to c:\\users\\roger\\appdata\\local\\temp\\pip-req-build-9b1i23vd\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from atari-py==1.2.2) (1.24.4)\n",
            "Building wheels for collected packages: atari-py\n",
            "  Building wheel for atari-py (setup.py): started\n",
            "  Building wheel for atari-py (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for atari-py\n",
            "Failed to build atari-py\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git 'C:\\Users\\Roger\\AppData\\Local\\Temp\\pip-req-build-9b1i23vd'\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  python setup.py bdist_wheel did not run successfully.\n",
            "  exit code: 1\n",
            "  \n",
            "  [75 lines of output]\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build\\lib.win-amd64-cpython-38\n",
            "  creating build\\lib.win-amd64-cpython-38\\atari_py\n",
            "  copying atari_py\\ale_python_interface.py -> build\\lib.win-amd64-cpython-38\\atari_py\n",
            "  copying atari_py\\__init__.py -> build\\lib.win-amd64-cpython-38\\atari_py\n",
            "  creating build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\adventure.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\air_raid.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\alien.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\amidar.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\assault.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\asterix.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\asteroids.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\atlantis.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\bank_heist.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\battle_zone.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\beam_rider.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\berzerk.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\bowling.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\boxing.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\breakout.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\carnival.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\centipede.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\chopper_command.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\crazy_climber.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\defender.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\demon_attack.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\double_dunk.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\elevator_action.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\enduro.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\fishing_derby.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\freeway.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\frostbite.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\gopher.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\gravitar.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\hero.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\ice_hockey.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\jamesbond.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\journey_escape.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\kaboom.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\kangaroo.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\krull.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\kung_fu_master.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\montezuma_revenge.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\ms_pacman.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\name_this_game.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\phoenix.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\pitfall.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\pong.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\pooyan.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\private_eye.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\qbert.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\riverraid.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\road_runner.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\robotank.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\seaquest.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\skiing.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\solaris.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\space_invaders.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\star_gunner.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\tennis.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\time_pilot.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\tutankham.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\up_n_down.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\venture.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\video_pinball.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\wizard_of_wor.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\yars_revenge.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  copying atari_py\\atari_roms\\zaxxon.bin -> build\\lib.win-amd64-cpython-38\\atari_py\\atari_roms\n",
            "  running build_ext\n",
            "  building 'ale_c' extension\n",
            "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "  [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for atari-py\n",
            "ERROR: Could not build wheels for atari-py, which is required to install pyproject.toml-based projects\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyglet==1.5.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (1.5.0)\n",
            "Requirement already satisfied: future in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from pyglet==1.5.0) (0.18.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py==3.1.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from h5py==3.1.0) (1.24.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow==9.5.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (9.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl2==1.0.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from keras-rl2==1.0.5) (2.5.3)\n",
            "Collecting numpy~=1.19.2 (from tensorflow->keras-rl2==1.0.5)\n",
            "  Using cached numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
            "Requirement already satisfied: absl-py~=0.10 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n",
            "Requirement already satisfied: six~=1.15.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.41.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.34.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.25.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (68.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (7.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "Successfully installed numpy-1.19.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-intel 2.13.0 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires flatbuffers>=23.1.21, but you have flatbuffers 1.12 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.11.2 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.5.0 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Keras==2.2.4\n",
            "  Using cached Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from Keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from Keras==2.2.4) (1.10.1)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from Keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from Keras==2.2.4) (6.0.1)\n",
            "Requirement already satisfied: h5py in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from Keras==2.2.4) (3.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from Keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from Keras==2.2.4) (1.1.2)\n",
            "Installing collected packages: Keras\n",
            "  Attempting uninstall: Keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "Successfully installed Keras-2.2.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-intel 2.13.0 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires flatbuffers>=23.1.21, but you have flatbuffers 1.12 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.2.4 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.19.5 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.11.2 which is incompatible.\n",
            "tensorflow-intel 2.13.0 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.5.0 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.5.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (2.5.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (1.19.5)\n",
            "Requirement already satisfied: absl-py~=0.10 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (0.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (3.20.3)\n",
            "Requirement already satisfied: six~=1.15.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (0.41.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow==2.5.3) (1.34.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.25.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (68.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (7.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow==2.5.3) (2.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from torch==2.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from torch==2.0.1) (3.7.4.3)\n",
            "Requirement already satisfied: sympy in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from torch==2.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from torch==2.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from torch==2.0.1) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: agents==1.4.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (1.4.0)\n",
            "Requirement already satisfied: tensorflow in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from agents==1.4.0) (2.5.3)\n",
            "Requirement already satisfied: gym in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from agents==1.4.0) (0.17.3)\n",
            "Requirement already satisfied: ruamel.yaml in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from agents==1.4.0) (0.18.5)\n",
            "Requirement already satisfied: scipy in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from gym->agents==1.4.0) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from gym->agents==1.4.0) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from gym->agents==1.4.0) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from gym->agents==1.4.0) (1.6.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from ruamel.yaml->agents==1.4.0) (0.2.8)\n",
            "Requirement already satisfied: absl-py~=0.10 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.20.3)\n",
            "Requirement already satisfied: six~=1.15.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.41.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.34.1)\n",
            "Requirement already satisfied: future in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym->agents==1.4.0) (0.18.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.25.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (68.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (7.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib==3.6 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.19 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from matplotlib==3.6) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.6) (1.15.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0\n",
        "  %pip install matplotlib==3.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClgEiM9jXCFZ",
        "outputId": "d2cfb768-6a1e-4fa4-87a7-79450919bef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Collecting joblib>=1.1.1 (from scikit-learn)\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
            "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/9.3 MB 991.0 kB/s eta 0:00:10\n",
            "   - -------------------------------------- 0.3/9.3 MB 3.8 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 0.7/9.3 MB 5.7 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 1.2/9.3 MB 6.8 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 1.7/9.3 MB 7.1 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 2.2/9.3 MB 7.8 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 2.8/9.3 MB 8.5 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 3.3/9.3 MB 8.9 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 3.9/9.3 MB 9.2 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 4.4/9.3 MB 9.5 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 5.0/9.3 MB 9.6 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 5.5/9.3 MB 9.8 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 6.1/9.3 MB 10.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 6.6/9.3 MB 10.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 7.2/9.3 MB 10.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 7.7/9.3 MB 10.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.2/9.3 MB 10.5 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 8.5/9.3 MB 10.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.3/9.3 MB 10.4 MB/s eta 0:00:00\n",
            "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 threadpoolctl-3.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\roger\\.conda\\envs\\miar_rl\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3eRhgI-Gb2a"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, LSTM, Reshape, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwOE6I_KGb2a",
        "outputId": "ae71a146-c572-49ba-8733-f7733645823c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hay 6 acciones\n",
            "Las acciones disponibles son NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE\n"
          ]
        }
      ],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "print(f\"Hay {nb_actions} acciones\")\n",
        "print(\"Las acciones disponibles son {}\".format(\", \".join(env.env.get_action_meanings())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzYO9vPuXCFa"
      },
      "outputs": [],
      "source": [
        "# Visualización del entorno\n",
        "from time import sleep\n",
        "def run_env(steps=100):\n",
        "    observation = env.reset()\n",
        "\n",
        "    for _ in range(steps):\n",
        "        # env.render()\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            observation = env.reset()\n",
        "    env.close()\n",
        "\n",
        "run_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQaSKvGtXCFa"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jGEZUcpGb2a"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        # Cortamos la observación para eliminar el score\n",
        "        # de arriba y el suelo de abajo\n",
        "        observation_cropped = observation[28:-14,4:-12]\n",
        "        # Convert observation to gray\n",
        "        img = Image.fromarray(observation_cropped)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyMFvUWGXCFa"
      },
      "outputs": [],
      "source": [
        "# Modificamos la classe EpsGreedyQPolicy para hacer que el epsilon\n",
        "# decaiga de 1 a 0.1 a lo largo de los 100 primeros intervalos.\n",
        "\n",
        "class EpsGreedyQPolicy(EpsGreedyQPolicy):\n",
        "    def __init__(self, eps=.1, eps_update=10000):\n",
        "        super().__init__(eps)\n",
        "        self.current_step = 0\n",
        "        self.eps_update = eps_update\n",
        "\n",
        "\n",
        "    def select_action(self, q_values):\n",
        "        self.current_step += 1\n",
        "        if not self.current_step % self.eps_update:\n",
        "            self.eps -= 0.02\n",
        "            if self.eps < .1:\n",
        "                self.eps = .1\n",
        "        return super().select_action(q_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "1. Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4GKrfWSGb2b",
        "outputId": "561c0283-5008-40a8-9df7-c3b66749a9ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "channels_last\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute (Permute)            (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        1184      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               590336    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 3078      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 949,606\n",
            "Trainable params: 949,606\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "print(K.image_data_format())\n",
        "if K.image_data_format() == 'channels_last':\n",
        "    # (width, height, channels)\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "elif K.image_data_format() == 'channels_first':\n",
        "    # (channels, width, height)\n",
        "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
        "else:\n",
        "    raise RuntimeError('Unknown image_dim_ordering.')\n",
        "\n",
        "model.add(Convolution2D(32, (3, 3), strides=(3, 3), padding=\"same\"))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Convolution2D(64, (3, 3), padding=\"same\"))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Convolution2D(128, (3, 3), padding=\"same\"))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "2. Implementación de la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foSlxWH1Gb2b"
      },
      "outputs": [],
      "source": [
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzsZD8sIXCFb"
      },
      "outputs": [],
      "source": [
        "policy = LinearAnnealedPolicy(\n",
        "    EpsGreedyQPolicy(eps=1.0),\n",
        "    attr='eps',\n",
        "    value_max=1.,\n",
        "    value_min=.1,\n",
        "    value_test=.05,\n",
        "    nb_steps=1000000\n",
        ")\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
        "               memory=memory, processor=processor,\n",
        "               # nb_steps_warmup=50000, No hace falta por la modificación en la policy\n",
        "               gamma=.97,\n",
        "               target_model_update=10000,\n",
        "               train_interval=WINDOW_LENGTH)\n",
        "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lBc2tkOXCFb",
        "outputId": "066fe956-77e0-42fa-fa8b-1e364f2ddbb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 2000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "   38/10000 [..............................] - ETA: 27s - reward: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Roger\\.conda\\envs\\miar_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0139\n",
            "15 episodes - episode_reward: 8.867 [3.000, 22.000] - loss: 0.006 - mae: 0.037 - mean_q: 0.074 - mean_eps: 0.995 - ale.lives: 2.133\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 191s 19ms/step - reward: 0.0141\n",
            "12 episodes - episode_reward: 11.333 [5.000, 22.000] - loss: 0.006 - mae: 0.061 - mean_q: 0.098 - mean_eps: 0.987 - ale.lives: 2.024\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 191s 19ms/step - reward: 0.0136\n",
            "16 episodes - episode_reward: 8.750 [4.000, 19.000] - loss: 0.007 - mae: 0.099 - mean_q: 0.136 - mean_eps: 0.978 - ale.lives: 2.206\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 192s 19ms/step - reward: 0.0139\n",
            "15 episodes - episode_reward: 9.467 [4.000, 28.000] - loss: 0.006 - mae: 0.101 - mean_q: 0.136 - mean_eps: 0.969 - ale.lives: 2.022\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 193s 19ms/step - reward: 0.0140\n",
            "13 episodes - episode_reward: 10.231 [4.000, 19.000] - loss: 0.007 - mae: 0.131 - mean_q: 0.169 - mean_eps: 0.960 - ale.lives: 2.184\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 194s 19ms/step - reward: 0.0143\n",
            "15 episodes - episode_reward: 9.400 [2.000, 16.000] - loss: 0.008 - mae: 0.169 - mean_q: 0.214 - mean_eps: 0.951 - ale.lives: 2.097\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 194s 19ms/step - reward: 0.0144\n",
            "16 episodes - episode_reward: 9.062 [3.000, 20.000] - loss: 0.008 - mae: 0.188 - mean_q: 0.235 - mean_eps: 0.942 - ale.lives: 2.045\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 195s 20ms/step - reward: 0.0137\n",
            "16 episodes - episode_reward: 8.562 [5.000, 16.000] - loss: 0.008 - mae: 0.201 - mean_q: 0.251 - mean_eps: 0.933 - ale.lives: 2.263\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 197s 20ms/step - reward: 0.0136\n",
            "16 episodes - episode_reward: 8.938 [2.000, 16.000] - loss: 0.008 - mae: 0.221 - mean_q: 0.273 - mean_eps: 0.924 - ale.lives: 2.119\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 198s 20ms/step - reward: 0.0125\n",
            "14 episodes - episode_reward: 8.500 [3.000, 18.000] - loss: 0.009 - mae: 0.243 - mean_q: 0.300 - mean_eps: 0.915 - ale.lives: 2.198\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 198s 20ms/step - reward: 0.0129\n",
            "14 episodes - episode_reward: 9.929 [1.000, 19.000] - loss: 0.009 - mae: 0.256 - mean_q: 0.315 - mean_eps: 0.906 - ale.lives: 2.154\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 200s 20ms/step - reward: 0.0142\n",
            "14 episodes - episode_reward: 9.929 [4.000, 17.000] - loss: 0.010 - mae: 0.295 - mean_q: 0.362 - mean_eps: 0.897 - ale.lives: 2.083\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 200s 20ms/step - reward: 0.0141\n",
            "14 episodes - episode_reward: 10.357 [4.000, 19.000] - loss: 0.009 - mae: 0.318 - mean_q: 0.389 - mean_eps: 0.888 - ale.lives: 2.088\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 202s 20ms/step - reward: 0.0160\n",
            "17 episodes - episode_reward: 8.765 [4.000, 25.000] - loss: 0.009 - mae: 0.335 - mean_q: 0.409 - mean_eps: 0.879 - ale.lives: 2.248\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 203s 20ms/step - reward: 0.0159\n",
            "13 episodes - episode_reward: 12.154 [6.000, 25.000] - loss: 0.009 - mae: 0.349 - mean_q: 0.425 - mean_eps: 0.870 - ale.lives: 1.954\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 204s 20ms/step - reward: 0.0142\n",
            "15 episodes - episode_reward: 9.667 [4.000, 20.000] - loss: 0.010 - mae: 0.367 - mean_q: 0.447 - mean_eps: 0.861 - ale.lives: 1.959\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 204s 20ms/step - reward: 0.0148\n",
            "15 episodes - episode_reward: 9.733 [4.000, 20.000] - loss: 0.009 - mae: 0.366 - mean_q: 0.445 - mean_eps: 0.852 - ale.lives: 2.163\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 205s 21ms/step - reward: 0.0151\n",
            "15 episodes - episode_reward: 10.067 [4.000, 23.000] - loss: 0.009 - mae: 0.375 - mean_q: 0.457 - mean_eps: 0.843 - ale.lives: 2.023\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 206s 21ms/step - reward: 0.0152\n",
            "16 episodes - episode_reward: 10.000 [4.000, 21.000] - loss: 0.010 - mae: 0.408 - mean_q: 0.497 - mean_eps: 0.834 - ale.lives: 2.089\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 209s 21ms/step - reward: 0.0171\n",
            "14 episodes - episode_reward: 11.786 [4.000, 32.000] - loss: 0.011 - mae: 0.425 - mean_q: 0.519 - mean_eps: 0.825 - ale.lives: 1.982\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0152\n",
            "14 episodes - episode_reward: 11.429 [3.000, 21.000] - loss: 0.010 - mae: 0.445 - mean_q: 0.543 - mean_eps: 0.816 - ale.lives: 2.171\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0165\n",
            "13 episodes - episode_reward: 11.846 [7.000, 21.000] - loss: 0.010 - mae: 0.455 - mean_q: 0.556 - mean_eps: 0.807 - ale.lives: 2.027\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0156\n",
            "15 episodes - episode_reward: 10.333 [3.000, 19.000] - loss: 0.012 - mae: 0.479 - mean_q: 0.585 - mean_eps: 0.798 - ale.lives: 2.080\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0169\n",
            "15 episodes - episode_reward: 11.533 [3.000, 20.000] - loss: 0.011 - mae: 0.504 - mean_q: 0.613 - mean_eps: 0.789 - ale.lives: 2.086\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 215s 22ms/step - reward: 0.0170\n",
            "13 episodes - episode_reward: 12.769 [6.000, 22.000] - loss: 0.012 - mae: 0.490 - mean_q: 0.599 - mean_eps: 0.780 - ale.lives: 2.097\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0167\n",
            "15 episodes - episode_reward: 11.667 [4.000, 19.000] - loss: 0.010 - mae: 0.521 - mean_q: 0.634 - mean_eps: 0.771 - ale.lives: 2.164\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0160\n",
            "16 episodes - episode_reward: 9.875 [5.000, 21.000] - loss: 0.010 - mae: 0.516 - mean_q: 0.629 - mean_eps: 0.762 - ale.lives: 2.064\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0159\n",
            "16 episodes - episode_reward: 9.812 [5.000, 25.000] - loss: 0.010 - mae: 0.517 - mean_q: 0.632 - mean_eps: 0.753 - ale.lives: 2.070\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0167\n",
            "15 episodes - episode_reward: 11.333 [5.000, 21.000] - loss: 0.010 - mae: 0.564 - mean_q: 0.688 - mean_eps: 0.744 - ale.lives: 2.211\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0173\n",
            "16 episodes - episode_reward: 10.688 [5.000, 21.000] - loss: 0.010 - mae: 0.577 - mean_q: 0.704 - mean_eps: 0.735 - ale.lives: 2.035\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0180\n",
            "14 episodes - episode_reward: 13.143 [7.000, 21.000] - loss: 0.010 - mae: 0.582 - mean_q: 0.710 - mean_eps: 0.726 - ale.lives: 2.193\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0162\n",
            "14 episodes - episode_reward: 11.571 [3.000, 25.000] - loss: 0.010 - mae: 0.593 - mean_q: 0.723 - mean_eps: 0.717 - ale.lives: 2.191\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0176\n",
            "14 episodes - episode_reward: 12.643 [5.000, 33.000] - loss: 0.010 - mae: 0.595 - mean_q: 0.723 - mean_eps: 0.708 - ale.lives: 2.127\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 226s 23ms/step - reward: 0.0154\n",
            "15 episodes - episode_reward: 10.467 [3.000, 16.000] - loss: 0.010 - mae: 0.600 - mean_q: 0.730 - mean_eps: 0.699 - ale.lives: 2.146\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0157\n",
            "15 episodes - episode_reward: 10.400 [6.000, 14.000] - loss: 0.009 - mae: 0.608 - mean_q: 0.738 - mean_eps: 0.690 - ale.lives: 2.128\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0166\n",
            "13 episodes - episode_reward: 12.308 [8.000, 18.000] - loss: 0.009 - mae: 0.615 - mean_q: 0.746 - mean_eps: 0.681 - ale.lives: 2.130\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 231s 23ms/step - reward: 0.0174\n",
            "13 episodes - episode_reward: 13.846 [6.000, 25.000] - loss: 0.009 - mae: 0.630 - mean_q: 0.765 - mean_eps: 0.672 - ale.lives: 2.059\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 236s 24ms/step - reward: 0.0165\n",
            "15 episodes - episode_reward: 9.933 [6.000, 13.000] - loss: 0.009 - mae: 0.626 - mean_q: 0.760 - mean_eps: 0.663 - ale.lives: 2.101\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 249s 25ms/step - reward: 0.0172\n",
            "14 episodes - episode_reward: 13.214 [7.000, 33.000] - loss: 0.010 - mae: 0.640 - mean_q: 0.777 - mean_eps: 0.654 - ale.lives: 2.091\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 256s 26ms/step - reward: 0.0162\n",
            "15 episodes - episode_reward: 10.533 [5.000, 14.000] - loss: 0.010 - mae: 0.639 - mean_q: 0.776 - mean_eps: 0.645 - ale.lives: 2.148\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 258s 26ms/step - reward: 0.0170\n",
            "15 episodes - episode_reward: 11.600 [7.000, 21.000] - loss: 0.010 - mae: 0.660 - mean_q: 0.803 - mean_eps: 0.636 - ale.lives: 2.156\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 260s 26ms/step - reward: 0.0167\n",
            "15 episodes - episode_reward: 10.667 [4.000, 16.000] - loss: 0.010 - mae: 0.680 - mean_q: 0.825 - mean_eps: 0.627 - ale.lives: 2.050\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 261s 26ms/step - reward: 0.0170\n",
            "15 episodes - episode_reward: 11.267 [7.000, 16.000] - loss: 0.009 - mae: 0.685 - mean_q: 0.831 - mean_eps: 0.618 - ale.lives: 2.186\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0183\n",
            "16 episodes - episode_reward: 11.438 [4.000, 27.000] - loss: 0.010 - mae: 0.704 - mean_q: 0.855 - mean_eps: 0.609 - ale.lives: 2.111\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0154\n",
            "16 episodes - episode_reward: 10.375 [2.000, 18.000] - loss: 0.010 - mae: 0.698 - mean_q: 0.847 - mean_eps: 0.600 - ale.lives: 2.103\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0180\n",
            "15 episodes - episode_reward: 11.200 [5.000, 25.000] - loss: 0.010 - mae: 0.716 - mean_q: 0.869 - mean_eps: 0.591 - ale.lives: 2.151\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 272s 27ms/step - reward: 0.0177\n",
            "14 episodes - episode_reward: 12.857 [8.000, 30.000] - loss: 0.010 - mae: 0.733 - mean_q: 0.890 - mean_eps: 0.582 - ale.lives: 2.254\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0190\n",
            "15 episodes - episode_reward: 13.000 [9.000, 20.000] - loss: 0.010 - mae: 0.732 - mean_q: 0.889 - mean_eps: 0.573 - ale.lives: 2.193\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0181\n",
            "14 episodes - episode_reward: 12.571 [7.000, 21.000] - loss: 0.010 - mae: 0.742 - mean_q: 0.900 - mean_eps: 0.564 - ale.lives: 2.152\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 278s 28ms/step - reward: 0.0169\n",
            "14 episodes - episode_reward: 12.214 [6.000, 21.000] - loss: 0.011 - mae: 0.752 - mean_q: 0.911 - mean_eps: 0.555 - ale.lives: 2.173\n",
            "\n",
            "Interval 51 (500000 steps performed)\n",
            "10000/10000 [==============================] - 282s 28ms/step - reward: 0.0165\n",
            "15 episodes - episode_reward: 11.200 [6.000, 17.000] - loss: 0.010 - mae: 0.737 - mean_q: 0.894 - mean_eps: 0.546 - ale.lives: 2.074\n",
            "\n",
            "Interval 52 (510000 steps performed)\n",
            "10000/10000 [==============================] - 285s 29ms/step - reward: 0.0179\n",
            "14 episodes - episode_reward: 12.571 [7.000, 22.000] - loss: 0.011 - mae: 0.756 - mean_q: 0.916 - mean_eps: 0.537 - ale.lives: 2.099\n",
            "\n",
            "Interval 53 (520000 steps performed)\n",
            "10000/10000 [==============================] - 288s 29ms/step - reward: 0.0195\n",
            "11 episodes - episode_reward: 18.182 [8.000, 27.000] - loss: 0.010 - mae: 0.764 - mean_q: 0.926 - mean_eps: 0.528 - ale.lives: 2.237\n",
            "\n",
            "Interval 54 (530000 steps performed)\n",
            "10000/10000 [==============================] - 290s 29ms/step - reward: 0.0165\n",
            "14 episodes - episode_reward: 11.214 [5.000, 23.000] - loss: 0.010 - mae: 0.778 - mean_q: 0.942 - mean_eps: 0.519 - ale.lives: 2.116\n",
            "\n",
            "Interval 55 (540000 steps performed)\n",
            "10000/10000 [==============================] - 292s 29ms/step - reward: 0.0170\n",
            "15 episodes - episode_reward: 12.000 [4.000, 26.000] - loss: 0.010 - mae: 0.782 - mean_q: 0.946 - mean_eps: 0.510 - ale.lives: 2.034\n",
            "\n",
            "Interval 56 (550000 steps performed)\n",
            "10000/10000 [==============================] - 295s 29ms/step - reward: 0.0183\n",
            "15 episodes - episode_reward: 11.800 [5.000, 24.000] - loss: 0.010 - mae: 0.799 - mean_q: 0.967 - mean_eps: 0.501 - ale.lives: 2.092\n",
            "\n",
            "Interval 57 (560000 steps performed)\n",
            "10000/10000 [==============================] - 298s 30ms/step - reward: 0.0189\n",
            "14 episodes - episode_reward: 13.071 [5.000, 26.000] - loss: 0.010 - mae: 0.796 - mean_q: 0.964 - mean_eps: 0.492 - ale.lives: 2.143\n",
            "\n",
            "Interval 58 (570000 steps performed)\n",
            "10000/10000 [==============================] - 302s 30ms/step - reward: 0.0197\n",
            "12 episodes - episode_reward: 16.417 [8.000, 23.000] - loss: 0.010 - mae: 0.801 - mean_q: 0.970 - mean_eps: 0.483 - ale.lives: 2.265\n",
            "\n",
            "Interval 59 (580000 steps performed)\n",
            "10000/10000 [==============================] - 305s 31ms/step - reward: 0.0177\n",
            "16 episodes - episode_reward: 11.812 [4.000, 18.000] - loss: 0.010 - mae: 0.806 - mean_q: 0.976 - mean_eps: 0.474 - ale.lives: 2.063\n",
            "\n",
            "Interval 60 (590000 steps performed)\n",
            "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0171\n",
            "12 episodes - episode_reward: 13.917 [7.000, 26.000] - loss: 0.010 - mae: 0.814 - mean_q: 0.986 - mean_eps: 0.465 - ale.lives: 1.986\n",
            "\n",
            "Interval 61 (600000 steps performed)\n",
            "10000/10000 [==============================] - 309s 31ms/step - reward: 0.0168\n",
            "15 episodes - episode_reward: 10.867 [6.000, 17.000] - loss: 0.010 - mae: 0.823 - mean_q: 0.997 - mean_eps: 0.456 - ale.lives: 2.161\n",
            "\n",
            "Interval 62 (610000 steps performed)\n",
            "10000/10000 [==============================] - 311s 31ms/step - reward: 0.0191\n",
            "16 episodes - episode_reward: 11.812 [6.000, 22.000] - loss: 0.011 - mae: 0.858 - mean_q: 1.040 - mean_eps: 0.447 - ale.lives: 2.206\n",
            "\n",
            "Interval 63 (620000 steps performed)\n",
            "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0174\n",
            "15 episodes - episode_reward: 12.333 [8.000, 19.000] - loss: 0.010 - mae: 0.857 - mean_q: 1.038 - mean_eps: 0.438 - ale.lives: 2.113\n",
            "\n",
            "Interval 64 (630000 steps performed)\n",
            "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0188\n",
            "13 episodes - episode_reward: 14.231 [8.000, 26.000] - loss: 0.010 - mae: 0.862 - mean_q: 1.043 - mean_eps: 0.429 - ale.lives: 2.138\n",
            "\n",
            "Interval 65 (640000 steps performed)\n",
            "10000/10000 [==============================] - 315s 32ms/step - reward: 0.0216\n",
            "14 episodes - episode_reward: 15.357 [9.000, 23.000] - loss: 0.010 - mae: 0.880 - mean_q: 1.064 - mean_eps: 0.420 - ale.lives: 2.055\n",
            "\n",
            "Interval 66 (650000 steps performed)\n",
            "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0207\n",
            "12 episodes - episode_reward: 15.667 [5.000, 28.000] - loss: 0.010 - mae: 0.867 - mean_q: 1.047 - mean_eps: 0.411 - ale.lives: 2.186\n",
            "\n",
            "Interval 67 (660000 steps performed)\n",
            "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0176\n",
            "16 episodes - episode_reward: 12.062 [6.000, 24.000] - loss: 0.011 - mae: 0.871 - mean_q: 1.053 - mean_eps: 0.402 - ale.lives: 2.132\n",
            "\n",
            "Interval 68 (670000 steps performed)\n",
            "10000/10000 [==============================] - 324s 32ms/step - reward: 0.0184\n",
            "14 episodes - episode_reward: 13.571 [7.000, 21.000] - loss: 0.010 - mae: 0.884 - mean_q: 1.068 - mean_eps: 0.393 - ale.lives: 2.127\n",
            "\n",
            "Interval 69 (680000 steps performed)\n",
            "10000/10000 [==============================] - 327s 33ms/step - reward: 0.0206\n",
            "11 episodes - episode_reward: 17.000 [8.000, 32.000] - loss: 0.011 - mae: 0.903 - mean_q: 1.091 - mean_eps: 0.384 - ale.lives: 1.972\n",
            "\n",
            "Interval 70 (690000 steps performed)\n",
            "10000/10000 [==============================] - 328s 33ms/step - reward: 0.0189\n",
            "13 episodes - episode_reward: 15.846 [5.000, 26.000] - loss: 0.010 - mae: 0.891 - mean_q: 1.077 - mean_eps: 0.375 - ale.lives: 2.217\n",
            "\n",
            "Interval 71 (700000 steps performed)\n",
            "10000/10000 [==============================] - 332s 33ms/step - reward: 0.0186\n",
            "13 episodes - episode_reward: 13.615 [7.000, 23.000] - loss: 0.010 - mae: 0.888 - mean_q: 1.074 - mean_eps: 0.366 - ale.lives: 2.082\n",
            "\n",
            "Interval 72 (710000 steps performed)\n",
            "10000/10000 [==============================] - 335s 34ms/step - reward: 0.0192\n",
            "13 episodes - episode_reward: 14.385 [8.000, 31.000] - loss: 0.012 - mae: 0.890 - mean_q: 1.075 - mean_eps: 0.357 - ale.lives: 2.189\n",
            "\n",
            "Interval 73 (720000 steps performed)\n",
            "10000/10000 [==============================] - 335s 33ms/step - reward: 0.0193\n",
            "12 episodes - episode_reward: 16.583 [8.000, 30.000] - loss: 0.009 - mae: 0.876 - mean_q: 1.058 - mean_eps: 0.348 - ale.lives: 2.027\n",
            "\n",
            "Interval 74 (730000 steps performed)\n",
            "10000/10000 [==============================] - 336s 34ms/step - reward: 0.0181\n",
            "14 episodes - episode_reward: 13.214 [7.000, 34.000] - loss: 0.011 - mae: 0.880 - mean_q: 1.063 - mean_eps: 0.339 - ale.lives: 2.004\n",
            "\n",
            "Interval 75 (740000 steps performed)\n",
            "10000/10000 [==============================] - 340s 34ms/step - reward: 0.0169\n",
            "12 episodes - episode_reward: 14.500 [7.000, 32.000] - loss: 0.012 - mae: 0.884 - mean_q: 1.067 - mean_eps: 0.330 - ale.lives: 2.291\n",
            "\n",
            "Interval 76 (750000 steps performed)\n",
            "10000/10000 [==============================] - 341s 34ms/step - reward: 0.0185\n",
            "12 episodes - episode_reward: 15.083 [5.000, 31.000] - loss: 0.010 - mae: 0.902 - mean_q: 1.088 - mean_eps: 0.321 - ale.lives: 2.148\n",
            "\n",
            "Interval 77 (760000 steps performed)\n",
            "10000/10000 [==============================] - 341s 34ms/step - reward: 0.0189\n",
            "13 episodes - episode_reward: 14.231 [4.000, 41.000] - loss: 0.011 - mae: 0.900 - mean_q: 1.086 - mean_eps: 0.312 - ale.lives: 2.216\n",
            "\n",
            "Interval 78 (770000 steps performed)\n",
            "10000/10000 [==============================] - 346s 35ms/step - reward: 0.0205\n",
            "13 episodes - episode_reward: 15.769 [4.000, 31.000] - loss: 0.010 - mae: 0.907 - mean_q: 1.095 - mean_eps: 0.303 - ale.lives: 2.195\n",
            "\n",
            "Interval 79 (780000 steps performed)\n",
            "10000/10000 [==============================] - 348s 35ms/step - reward: 0.0198\n",
            "12 episodes - episode_reward: 16.167 [5.000, 26.000] - loss: 0.010 - mae: 0.904 - mean_q: 1.090 - mean_eps: 0.294 - ale.lives: 2.198\n",
            "\n",
            "Interval 80 (790000 steps performed)\n",
            "10000/10000 [==============================] - 351s 35ms/step - reward: 0.0198\n",
            "13 episodes - episode_reward: 15.000 [9.000, 26.000] - loss: 0.010 - mae: 0.896 - mean_q: 1.082 - mean_eps: 0.285 - ale.lives: 2.131\n",
            "\n",
            "Interval 81 (800000 steps performed)\n",
            "10000/10000 [==============================] - 356s 36ms/step - reward: 0.0192\n",
            "14 episodes - episode_reward: 14.357 [8.000, 28.000] - loss: 0.011 - mae: 0.887 - mean_q: 1.071 - mean_eps: 0.276 - ale.lives: 2.131\n",
            "\n",
            "Interval 82 (810000 steps performed)\n",
            "10000/10000 [==============================] - 356s 36ms/step - reward: 0.0195\n",
            "12 episodes - episode_reward: 15.833 [8.000, 32.000] - loss: 0.010 - mae: 0.884 - mean_q: 1.067 - mean_eps: 0.267 - ale.lives: 2.159\n",
            "\n",
            "Interval 83 (820000 steps performed)\n",
            "10000/10000 [==============================] - 359s 36ms/step - reward: 0.0179\n",
            "14 episodes - episode_reward: 11.857 [6.000, 18.000] - loss: 0.010 - mae: 0.898 - mean_q: 1.084 - mean_eps: 0.258 - ale.lives: 2.112\n",
            "\n",
            "Interval 84 (830000 steps performed)\n",
            "10000/10000 [==============================] - 362s 36ms/step - reward: 0.0179\n",
            "14 episodes - episode_reward: 14.143 [4.000, 28.000] - loss: 0.012 - mae: 0.913 - mean_q: 1.102 - mean_eps: 0.249 - ale.lives: 2.050\n",
            "\n",
            "Interval 85 (840000 steps performed)\n",
            "10000/10000 [==============================] - 364s 36ms/step - reward: 0.0193\n",
            "14 episodes - episode_reward: 14.143 [8.000, 22.000] - loss: 0.010 - mae: 0.921 - mean_q: 1.111 - mean_eps: 0.240 - ale.lives: 2.008\n",
            "\n",
            "Interval 86 (850000 steps performed)\n",
            "10000/10000 [==============================] - 365s 37ms/step - reward: 0.0192\n",
            "9 episodes - episode_reward: 20.444 [6.000, 32.000] - loss: 0.010 - mae: 0.923 - mean_q: 1.112 - mean_eps: 0.231 - ale.lives: 2.148\n",
            "\n",
            "Interval 87 (860000 steps performed)\n",
            "10000/10000 [==============================] - 369s 37ms/step - reward: 0.0194\n",
            "10 episodes - episode_reward: 17.600 [11.000, 34.000] - loss: 0.010 - mae: 0.912 - mean_q: 1.099 - mean_eps: 0.222 - ale.lives: 2.104\n",
            "\n",
            "Interval 88 (870000 steps performed)\n",
            "10000/10000 [==============================] - 370s 37ms/step - reward: 0.0182\n",
            "15 episodes - episode_reward: 13.933 [4.000, 35.000] - loss: 0.011 - mae: 0.918 - mean_q: 1.107 - mean_eps: 0.213 - ale.lives: 2.067\n",
            "\n",
            "Interval 89 (880000 steps performed)\n",
            "10000/10000 [==============================] - 372s 37ms/step - reward: 0.0201\n",
            "12 episodes - episode_reward: 16.667 [5.000, 29.000] - loss: 0.010 - mae: 0.927 - mean_q: 1.118 - mean_eps: 0.204 - ale.lives: 2.047\n",
            "\n",
            "Interval 90 (890000 steps performed)\n",
            "10000/10000 [==============================] - 377s 38ms/step - reward: 0.0186\n",
            "15 episodes - episode_reward: 12.467 [4.000, 20.000] - loss: 0.010 - mae: 0.931 - mean_q: 1.122 - mean_eps: 0.195 - ale.lives: 2.047\n",
            "\n",
            "Interval 91 (900000 steps performed)\n",
            "10000/10000 [==============================] - 377s 38ms/step - reward: 0.0181\n",
            "14 episodes - episode_reward: 12.929 [5.000, 22.000] - loss: 0.010 - mae: 0.935 - mean_q: 1.127 - mean_eps: 0.186 - ale.lives: 2.076\n",
            "\n",
            "Interval 92 (910000 steps performed)\n",
            "10000/10000 [==============================] - 380s 38ms/step - reward: 0.0187\n",
            "13 episodes - episode_reward: 13.846 [6.000, 29.000] - loss: 0.012 - mae: 0.933 - mean_q: 1.126 - mean_eps: 0.177 - ale.lives: 2.349\n",
            "\n",
            "Interval 93 (920000 steps performed)\n",
            "10000/10000 [==============================] - 382s 38ms/step - reward: 0.0172\n",
            "13 episodes - episode_reward: 12.538 [7.000, 21.000] - loss: 0.010 - mae: 0.933 - mean_q: 1.125 - mean_eps: 0.168 - ale.lives: 2.147\n",
            "\n",
            "Interval 94 (930000 steps performed)\n",
            "10000/10000 [==============================] - 388s 39ms/step - reward: 0.0184\n",
            "13 episodes - episode_reward: 14.538 [6.000, 28.000] - loss: 0.010 - mae: 0.935 - mean_q: 1.126 - mean_eps: 0.159 - ale.lives: 2.077\n",
            "\n",
            "Interval 95 (940000 steps performed)\n",
            "10000/10000 [==============================] - 386s 39ms/step - reward: 0.0197\n",
            "13 episodes - episode_reward: 14.385 [5.000, 26.000] - loss: 0.010 - mae: 0.934 - mean_q: 1.127 - mean_eps: 0.150 - ale.lives: 2.058\n",
            "\n",
            "Interval 96 (950000 steps performed)\n",
            "10000/10000 [==============================] - 394s 39ms/step - reward: 0.0203\n",
            "13 episodes - episode_reward: 15.308 [6.000, 29.000] - loss: 0.010 - mae: 0.931 - mean_q: 1.123 - mean_eps: 0.141 - ale.lives: 2.182\n",
            "\n",
            "Interval 97 (960000 steps performed)\n",
            "10000/10000 [==============================] - 394s 39ms/step - reward: 0.0186\n",
            "15 episodes - episode_reward: 13.800 [6.000, 28.000] - loss: 0.012 - mae: 0.952 - mean_q: 1.148 - mean_eps: 0.132 - ale.lives: 2.133\n",
            "\n",
            "Interval 98 (970000 steps performed)\n",
            "10000/10000 [==============================] - 397s 40ms/step - reward: 0.0190\n",
            "14 episodes - episode_reward: 13.857 [7.000, 25.000] - loss: 0.013 - mae: 0.948 - mean_q: 1.142 - mean_eps: 0.123 - ale.lives: 2.123\n",
            "\n",
            "Interval 99 (980000 steps performed)\n",
            "10000/10000 [==============================] - 398s 40ms/step - reward: 0.0206\n",
            "11 episodes - episode_reward: 18.000 [4.000, 33.000] - loss: 0.014 - mae: 0.957 - mean_q: 1.155 - mean_eps: 0.114 - ale.lives: 2.150\n",
            "\n",
            "Interval 100 (990000 steps performed)\n",
            "10000/10000 [==============================] - 402s 40ms/step - reward: 0.0208\n",
            "12 episodes - episode_reward: 16.167 [8.000, 24.000] - loss: 0.010 - mae: 0.953 - mean_q: 1.147 - mean_eps: 0.105 - ale.lives: 2.242\n",
            "\n",
            "Interval 101 (1000000 steps performed)\n",
            "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0196\n",
            "12 episodes - episode_reward: 16.583 [3.000, 30.000] - loss: 0.016 - mae: 0.947 - mean_q: 1.141 - mean_eps: 0.100 - ale.lives: 2.212\n",
            "\n",
            "Interval 102 (1010000 steps performed)\n",
            "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0196\n",
            "13 episodes - episode_reward: 16.538 [7.000, 22.000] - loss: 0.009 - mae: 0.947 - mean_q: 1.143 - mean_eps: 0.100 - ale.lives: 2.023\n",
            "\n",
            "Interval 103 (1020000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0193\n",
            "13 episodes - episode_reward: 14.077 [3.000, 28.000] - loss: 0.010 - mae: 0.939 - mean_q: 1.132 - mean_eps: 0.100 - ale.lives: 2.127\n",
            "\n",
            "Interval 104 (1030000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0204\n",
            "12 episodes - episode_reward: 17.167 [8.000, 27.000] - loss: 0.012 - mae: 0.931 - mean_q: 1.124 - mean_eps: 0.100 - ale.lives: 2.148\n",
            "\n",
            "Interval 105 (1040000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0203\n",
            "12 episodes - episode_reward: 17.167 [8.000, 27.000] - loss: 0.010 - mae: 0.936 - mean_q: 1.127 - mean_eps: 0.100 - ale.lives: 2.090\n",
            "\n",
            "Interval 106 (1050000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0190\n",
            "12 episodes - episode_reward: 16.250 [7.000, 28.000] - loss: 0.010 - mae: 0.944 - mean_q: 1.137 - mean_eps: 0.100 - ale.lives: 2.155\n",
            "\n",
            "Interval 107 (1060000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0195\n",
            "11 episodes - episode_reward: 17.636 [4.000, 28.000] - loss: 0.011 - mae: 0.942 - mean_q: 1.135 - mean_eps: 0.100 - ale.lives: 2.070\n",
            "\n",
            "Interval 108 (1070000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0188\n",
            "12 episodes - episode_reward: 15.417 [6.000, 31.000] - loss: 0.009 - mae: 0.934 - mean_q: 1.125 - mean_eps: 0.100 - ale.lives: 2.234\n",
            "\n",
            "Interval 109 (1080000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0227\n",
            "12 episodes - episode_reward: 18.083 [7.000, 27.000] - loss: 0.009 - mae: 0.932 - mean_q: 1.124 - mean_eps: 0.100 - ale.lives: 2.189\n",
            "\n",
            "Interval 110 (1090000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0191\n",
            "13 episodes - episode_reward: 14.462 [5.000, 22.000] - loss: 0.009 - mae: 0.936 - mean_q: 1.128 - mean_eps: 0.100 - ale.lives: 2.216\n",
            "\n",
            "Interval 111 (1100000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0195\n",
            "12 episodes - episode_reward: 16.833 [9.000, 25.000] - loss: 0.010 - mae: 0.939 - mean_q: 1.131 - mean_eps: 0.100 - ale.lives: 2.064\n",
            "\n",
            "Interval 112 (1110000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0193\n",
            "14 episodes - episode_reward: 14.357 [8.000, 31.000] - loss: 0.010 - mae: 0.950 - mean_q: 1.144 - mean_eps: 0.100 - ale.lives: 2.198\n",
            "\n",
            "Interval 113 (1120000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0226\n",
            "12 episodes - episode_reward: 17.583 [9.000, 32.000] - loss: 0.010 - mae: 0.936 - mean_q: 1.127 - mean_eps: 0.100 - ale.lives: 2.178\n",
            "\n",
            "Interval 114 (1130000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0218\n",
            "11 episodes - episode_reward: 20.636 [6.000, 35.000] - loss: 0.010 - mae: 0.927 - mean_q: 1.117 - mean_eps: 0.100 - ale.lives: 2.131\n",
            "\n",
            "Interval 115 (1140000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0202\n",
            "13 episodes - episode_reward: 14.462 [8.000, 25.000] - loss: 0.010 - mae: 0.939 - mean_q: 1.131 - mean_eps: 0.100 - ale.lives: 2.026\n",
            "\n",
            "Interval 116 (1150000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0222\n",
            "11 episodes - episode_reward: 19.545 [9.000, 36.000] - loss: 0.009 - mae: 0.942 - mean_q: 1.134 - mean_eps: 0.100 - ale.lives: 2.071\n",
            "\n",
            "Interval 117 (1160000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0188\n",
            "12 episodes - episode_reward: 17.667 [6.000, 32.000] - loss: 0.010 - mae: 0.937 - mean_q: 1.128 - mean_eps: 0.100 - ale.lives: 1.898\n",
            "\n",
            "Interval 118 (1170000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0206\n",
            "12 episodes - episode_reward: 16.333 [5.000, 28.000] - loss: 0.009 - mae: 0.936 - mean_q: 1.127 - mean_eps: 0.100 - ale.lives: 1.992\n",
            "\n",
            "Interval 119 (1180000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0211\n",
            "13 episodes - episode_reward: 16.846 [10.000, 27.000] - loss: 0.009 - mae: 0.924 - mean_q: 1.113 - mean_eps: 0.100 - ale.lives: 1.992\n",
            "\n",
            "Interval 120 (1190000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0188\n",
            "13 episodes - episode_reward: 14.462 [4.000, 24.000] - loss: 0.009 - mae: 0.933 - mean_q: 1.124 - mean_eps: 0.100 - ale.lives: 2.049\n",
            "\n",
            "Interval 121 (1200000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0218\n",
            "13 episodes - episode_reward: 16.154 [7.000, 22.000] - loss: 0.010 - mae: 0.948 - mean_q: 1.141 - mean_eps: 0.100 - ale.lives: 2.067\n",
            "\n",
            "Interval 122 (1210000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0214\n",
            "13 episodes - episode_reward: 16.692 [5.000, 30.000] - loss: 0.010 - mae: 0.927 - mean_q: 1.116 - mean_eps: 0.100 - ale.lives: 2.187\n",
            "\n",
            "Interval 123 (1220000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0200\n",
            "12 episodes - episode_reward: 17.000 [8.000, 27.000] - loss: 0.009 - mae: 0.921 - mean_q: 1.109 - mean_eps: 0.100 - ale.lives: 1.992\n",
            "\n",
            "Interval 124 (1230000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0208\n",
            "12 episodes - episode_reward: 17.667 [9.000, 26.000] - loss: 0.010 - mae: 0.931 - mean_q: 1.121 - mean_eps: 0.100 - ale.lives: 2.136\n",
            "\n",
            "Interval 125 (1240000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0196\n",
            "12 episodes - episode_reward: 16.167 [8.000, 29.000] - loss: 0.010 - mae: 0.933 - mean_q: 1.123 - mean_eps: 0.100 - ale.lives: 2.059\n",
            "\n",
            "Interval 126 (1250000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0215\n",
            "11 episodes - episode_reward: 18.545 [5.000, 33.000] - loss: 0.009 - mae: 0.932 - mean_q: 1.122 - mean_eps: 0.100 - ale.lives: 2.229\n",
            "\n",
            "Interval 127 (1260000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0167\n",
            "14 episodes - episode_reward: 12.214 [4.000, 25.000] - loss: 0.009 - mae: 0.915 - mean_q: 1.101 - mean_eps: 0.100 - ale.lives: 2.093\n",
            "\n",
            "Interval 128 (1270000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0219\n",
            "12 episodes - episode_reward: 17.083 [6.000, 29.000] - loss: 0.009 - mae: 0.908 - mean_q: 1.093 - mean_eps: 0.100 - ale.lives: 1.979\n",
            "\n",
            "Interval 129 (1280000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0200\n",
            "11 episodes - episode_reward: 19.818 [8.000, 32.000] - loss: 0.009 - mae: 0.913 - mean_q: 1.099 - mean_eps: 0.100 - ale.lives: 2.243\n",
            "\n",
            "Interval 130 (1290000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0200\n",
            "13 episodes - episode_reward: 15.615 [10.000, 25.000] - loss: 0.009 - mae: 0.914 - mean_q: 1.101 - mean_eps: 0.100 - ale.lives: 2.125\n",
            "\n",
            "Interval 131 (1300000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0206\n",
            "12 episodes - episode_reward: 17.667 [6.000, 33.000] - loss: 0.010 - mae: 0.921 - mean_q: 1.109 - mean_eps: 0.100 - ale.lives: 2.152\n",
            "\n",
            "Interval 132 (1310000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0193\n",
            "12 episodes - episode_reward: 14.417 [5.000, 27.000] - loss: 0.010 - mae: 0.918 - mean_q: 1.104 - mean_eps: 0.100 - ale.lives: 2.216\n",
            "\n",
            "Interval 133 (1320000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0202\n",
            "11 episodes - episode_reward: 20.091 [13.000, 28.000] - loss: 0.010 - mae: 0.921 - mean_q: 1.108 - mean_eps: 0.100 - ale.lives: 1.940\n",
            "\n",
            "Interval 134 (1330000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0222\n",
            "11 episodes - episode_reward: 18.727 [8.000, 30.000] - loss: 0.010 - mae: 0.924 - mean_q: 1.112 - mean_eps: 0.100 - ale.lives: 2.170\n",
            "\n",
            "Interval 135 (1340000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0204\n",
            "13 episodes - episode_reward: 16.077 [7.000, 27.000] - loss: 0.010 - mae: 0.914 - mean_q: 1.100 - mean_eps: 0.100 - ale.lives: 2.062\n",
            "\n",
            "Interval 136 (1350000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0194\n",
            "14 episodes - episode_reward: 14.214 [4.000, 26.000] - loss: 0.010 - mae: 0.921 - mean_q: 1.108 - mean_eps: 0.100 - ale.lives: 2.083\n",
            "\n",
            "Interval 137 (1360000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0173\n",
            "14 episodes - episode_reward: 12.786 [5.000, 25.000] - loss: 0.010 - mae: 0.922 - mean_q: 1.109 - mean_eps: 0.100 - ale.lives: 2.111\n",
            "\n",
            "Interval 138 (1370000 steps performed)\n",
            "10000/10000 [==============================] - 401s 40ms/step - reward: 0.0199\n",
            "13 episodes - episode_reward: 15.000 [8.000, 20.000] - loss: 0.010 - mae: 0.924 - mean_q: 1.112 - mean_eps: 0.100 - ale.lives: 2.076\n",
            "\n",
            "Interval 139 (1380000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0217\n",
            "12 episodes - episode_reward: 18.500 [5.000, 28.000] - loss: 0.010 - mae: 0.925 - mean_q: 1.113 - mean_eps: 0.100 - ale.lives: 2.080\n",
            "\n",
            "Interval 140 (1390000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0205\n",
            "12 episodes - episode_reward: 15.333 [4.000, 25.000] - loss: 0.010 - mae: 0.914 - mean_q: 1.100 - mean_eps: 0.100 - ale.lives: 2.145\n",
            "\n",
            "Interval 141 (1400000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0203\n",
            "12 episodes - episode_reward: 18.250 [9.000, 29.000] - loss: 0.010 - mae: 0.915 - mean_q: 1.101 - mean_eps: 0.100 - ale.lives: 2.069\n",
            "\n",
            "Interval 142 (1410000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0201\n",
            "10 episodes - episode_reward: 19.400 [7.000, 25.000] - loss: 0.010 - mae: 0.931 - mean_q: 1.121 - mean_eps: 0.100 - ale.lives: 2.148\n",
            "\n",
            "Interval 143 (1420000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0214\n",
            "13 episodes - episode_reward: 16.923 [8.000, 34.000] - loss: 0.010 - mae: 0.925 - mean_q: 1.113 - mean_eps: 0.100 - ale.lives: 2.037\n",
            "\n",
            "Interval 144 (1430000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0207\n",
            "12 episodes - episode_reward: 15.833 [6.000, 26.000] - loss: 0.010 - mae: 0.935 - mean_q: 1.123 - mean_eps: 0.100 - ale.lives: 1.990\n",
            "\n",
            "Interval 145 (1440000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0203\n",
            "13 episodes - episode_reward: 15.615 [7.000, 29.000] - loss: 0.009 - mae: 0.925 - mean_q: 1.113 - mean_eps: 0.100 - ale.lives: 2.252\n",
            "\n",
            "Interval 146 (1450000 steps performed)\n",
            "10000/10000 [==============================] - 405s 41ms/step - reward: 0.0209\n",
            "13 episodes - episode_reward: 16.923 [4.000, 30.000] - loss: 0.010 - mae: 0.932 - mean_q: 1.122 - mean_eps: 0.100 - ale.lives: 2.182\n",
            "\n",
            "Interval 147 (1460000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0210\n",
            "12 episodes - episode_reward: 16.750 [3.000, 29.000] - loss: 0.010 - mae: 0.932 - mean_q: 1.122 - mean_eps: 0.100 - ale.lives: 2.136\n",
            "\n",
            "Interval 148 (1470000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0199\n",
            "13 episodes - episode_reward: 16.692 [4.000, 27.000] - loss: 0.009 - mae: 0.923 - mean_q: 1.110 - mean_eps: 0.100 - ale.lives: 2.023\n",
            "\n",
            "Interval 149 (1480000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0190\n",
            "12 episodes - episode_reward: 14.917 [8.000, 27.000] - loss: 0.010 - mae: 0.916 - mean_q: 1.102 - mean_eps: 0.100 - ale.lives: 2.036\n",
            "\n",
            "Interval 150 (1490000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0181\n",
            "13 episodes - episode_reward: 14.538 [3.000, 25.000] - loss: 0.010 - mae: 0.915 - mean_q: 1.099 - mean_eps: 0.100 - ale.lives: 2.043\n",
            "\n",
            "Interval 151 (1500000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0217\n",
            "12 episodes - episode_reward: 17.417 [8.000, 31.000] - loss: 0.010 - mae: 0.936 - mean_q: 1.126 - mean_eps: 0.100 - ale.lives: 2.177\n",
            "\n",
            "Interval 152 (1510000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0209\n",
            "13 episodes - episode_reward: 15.769 [3.000, 27.000] - loss: 0.009 - mae: 0.932 - mean_q: 1.121 - mean_eps: 0.100 - ale.lives: 2.134\n",
            "\n",
            "Interval 153 (1520000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0221\n",
            "14 episodes - episode_reward: 16.143 [10.000, 27.000] - loss: 0.010 - mae: 0.931 - mean_q: 1.120 - mean_eps: 0.100 - ale.lives: 2.256\n",
            "\n",
            "Interval 154 (1530000 steps performed)\n",
            "10000/10000 [==============================] - 402s 40ms/step - reward: 0.0212\n",
            "13 episodes - episode_reward: 16.615 [8.000, 32.000] - loss: 0.010 - mae: 0.926 - mean_q: 1.114 - mean_eps: 0.100 - ale.lives: 2.152\n",
            "\n",
            "Interval 155 (1540000 steps performed)\n",
            "10000/10000 [==============================] - 402s 40ms/step - reward: 0.0196\n",
            "12 episodes - episode_reward: 15.500 [10.000, 24.000] - loss: 0.010 - mae: 0.913 - mean_q: 1.099 - mean_eps: 0.100 - ale.lives: 2.085\n",
            "\n",
            "Interval 156 (1550000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0216\n",
            "12 episodes - episode_reward: 19.167 [10.000, 31.000] - loss: 0.010 - mae: 0.910 - mean_q: 1.095 - mean_eps: 0.100 - ale.lives: 2.315\n",
            "\n",
            "Interval 157 (1560000 steps performed)\n",
            "10000/10000 [==============================] - 402s 40ms/step - reward: 0.0215\n",
            "12 episodes - episode_reward: 17.750 [3.000, 25.000] - loss: 0.010 - mae: 0.917 - mean_q: 1.103 - mean_eps: 0.100 - ale.lives: 2.136\n",
            "\n",
            "Interval 158 (1570000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0235\n",
            "11 episodes - episode_reward: 19.545 [10.000, 29.000] - loss: 0.009 - mae: 0.913 - mean_q: 1.100 - mean_eps: 0.100 - ale.lives: 2.050\n",
            "\n",
            "Interval 159 (1580000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0212\n",
            "12 episodes - episode_reward: 19.333 [9.000, 27.000] - loss: 0.010 - mae: 0.914 - mean_q: 1.100 - mean_eps: 0.100 - ale.lives: 2.168\n",
            "\n",
            "Interval 160 (1590000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0222\n",
            "11 episodes - episode_reward: 19.455 [7.000, 34.000] - loss: 0.010 - mae: 0.914 - mean_q: 1.100 - mean_eps: 0.100 - ale.lives: 2.385\n",
            "\n",
            "Interval 161 (1600000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0197\n",
            "14 episodes - episode_reward: 14.857 [6.000, 28.000] - loss: 0.009 - mae: 0.917 - mean_q: 1.104 - mean_eps: 0.100 - ale.lives: 1.963\n",
            "\n",
            "Interval 162 (1610000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0224\n",
            "13 episodes - episode_reward: 17.462 [4.000, 29.000] - loss: 0.010 - mae: 0.924 - mean_q: 1.112 - mean_eps: 0.100 - ale.lives: 2.190\n",
            "\n",
            "Interval 163 (1620000 steps performed)\n",
            "10000/10000 [==============================] - 402s 40ms/step - reward: 0.0213\n",
            "11 episodes - episode_reward: 18.636 [6.000, 34.000] - loss: 0.010 - mae: 0.917 - mean_q: 1.104 - mean_eps: 0.100 - ale.lives: 2.088\n",
            "\n",
            "Interval 164 (1630000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0220\n",
            "12 episodes - episode_reward: 18.917 [10.000, 27.000] - loss: 0.010 - mae: 0.928 - mean_q: 1.116 - mean_eps: 0.100 - ale.lives: 1.951\n",
            "\n",
            "Interval 165 (1640000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0226\n",
            "11 episodes - episode_reward: 20.000 [12.000, 28.000] - loss: 0.010 - mae: 0.938 - mean_q: 1.128 - mean_eps: 0.100 - ale.lives: 2.144\n",
            "\n",
            "Interval 166 (1650000 steps performed)\n",
            "10000/10000 [==============================] - 403s 40ms/step - reward: 0.0211\n",
            "12 episodes - episode_reward: 17.333 [11.000, 28.000] - loss: 0.010 - mae: 0.933 - mean_q: 1.123 - mean_eps: 0.100 - ale.lives: 2.226\n",
            "\n",
            "Interval 167 (1660000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0220\n",
            "13 episodes - episode_reward: 17.769 [11.000, 29.000] - loss: 0.010 - mae: 0.944 - mean_q: 1.136 - mean_eps: 0.100 - ale.lives: 2.001\n",
            "\n",
            "Interval 168 (1670000 steps performed)\n",
            "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0192\n",
            "13 episodes - episode_reward: 13.692 [4.000, 24.000] - loss: 0.010 - mae: 0.954 - mean_q: 1.149 - mean_eps: 0.100 - ale.lives: 2.115\n",
            "\n",
            "Interval 169 (1680000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0202\n",
            "14 episodes - episode_reward: 14.929 [4.000, 29.000] - loss: 0.010 - mae: 0.948 - mean_q: 1.142 - mean_eps: 0.100 - ale.lives: 2.097\n",
            "\n",
            "Interval 170 (1690000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0228\n",
            "12 episodes - episode_reward: 19.333 [8.000, 33.000] - loss: 0.010 - mae: 0.944 - mean_q: 1.137 - mean_eps: 0.100 - ale.lives: 2.141\n",
            "\n",
            "Interval 171 (1700000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0212\n",
            "11 episodes - episode_reward: 19.545 [10.000, 27.000] - loss: 0.010 - mae: 0.947 - mean_q: 1.141 - mean_eps: 0.100 - ale.lives: 1.912\n",
            "\n",
            "Interval 172 (1710000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0225\n",
            "14 episodes - episode_reward: 15.500 [8.000, 31.000] - loss: 0.010 - mae: 0.948 - mean_q: 1.142 - mean_eps: 0.100 - ale.lives: 2.119\n",
            "\n",
            "Interval 173 (1720000 steps performed)\n",
            "10000/10000 [==============================] - 405s 41ms/step - reward: 0.0213\n",
            "12 episodes - episode_reward: 15.917 [6.000, 30.000] - loss: 0.010 - mae: 0.945 - mean_q: 1.138 - mean_eps: 0.100 - ale.lives: 2.249\n",
            "\n",
            "Interval 174 (1730000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0225\n",
            "12 episodes - episode_reward: 20.583 [9.000, 30.000] - loss: 0.010 - mae: 0.951 - mean_q: 1.145 - mean_eps: 0.100 - ale.lives: 2.048\n",
            "\n",
            "Interval 175 (1740000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0241\n",
            "12 episodes - episode_reward: 20.000 [7.000, 27.000] - loss: 0.011 - mae: 0.957 - mean_q: 1.152 - mean_eps: 0.100 - ale.lives: 2.229\n",
            "\n",
            "Interval 176 (1750000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0227\n",
            "10 episodes - episode_reward: 22.200 [15.000, 34.000] - loss: 0.011 - mae: 0.944 - mean_q: 1.137 - mean_eps: 0.100 - ale.lives: 1.991\n",
            "\n",
            "Interval 177 (1760000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0212\n",
            "12 episodes - episode_reward: 17.333 [7.000, 31.000] - loss: 0.011 - mae: 0.936 - mean_q: 1.125 - mean_eps: 0.100 - ale.lives: 2.075\n",
            "\n",
            "Interval 178 (1770000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0232\n",
            "14 episodes - episode_reward: 17.214 [6.000, 33.000] - loss: 0.010 - mae: 0.927 - mean_q: 1.115 - mean_eps: 0.100 - ale.lives: 2.316\n",
            "\n",
            "Interval 179 (1780000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0221\n",
            "11 episodes - episode_reward: 18.636 [10.000, 27.000] - loss: 0.010 - mae: 0.930 - mean_q: 1.119 - mean_eps: 0.100 - ale.lives: 2.180\n",
            "\n",
            "Interval 180 (1790000 steps performed)\n",
            "10000/10000 [==============================] - 404s 40ms/step - reward: 0.0197\n",
            "12 episodes - episode_reward: 16.917 [10.000, 32.000] - loss: 0.010 - mae: 0.926 - mean_q: 1.113 - mean_eps: 0.100 - ale.lives: 2.191\n",
            "\n",
            "Interval 181 (1800000 steps performed)\n",
            "10000/10000 [==============================] - 405s 41ms/step - reward: 0.0222\n",
            "12 episodes - episode_reward: 19.917 [9.000, 35.000] - loss: 0.010 - mae: 0.911 - mean_q: 1.097 - mean_eps: 0.100 - ale.lives: 2.000\n",
            "\n",
            "Interval 182 (1810000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0225\n",
            "11 episodes - episode_reward: 20.455 [3.000, 33.000] - loss: 0.010 - mae: 0.911 - mean_q: 1.097 - mean_eps: 0.100 - ale.lives: 2.052\n",
            "\n",
            "Interval 183 (1820000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0219\n",
            "13 episodes - episode_reward: 15.692 [7.000, 30.000] - loss: 0.010 - mae: 0.915 - mean_q: 1.101 - mean_eps: 0.100 - ale.lives: 2.067\n",
            "\n",
            "Interval 184 (1830000 steps performed)\n",
            "10000/10000 [==============================] - 417s 42ms/step - reward: 0.0207\n",
            "10 episodes - episode_reward: 19.900 [8.000, 32.000] - loss: 0.010 - mae: 0.916 - mean_q: 1.101 - mean_eps: 0.100 - ale.lives: 2.117\n",
            "\n",
            "Interval 185 (1840000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0196\n",
            "13 episodes - episode_reward: 16.462 [6.000, 26.000] - loss: 0.010 - mae: 0.920 - mean_q: 1.106 - mean_eps: 0.100 - ale.lives: 2.103\n",
            "\n",
            "Interval 186 (1850000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0219\n",
            "12 episodes - episode_reward: 17.583 [5.000, 34.000] - loss: 0.010 - mae: 0.906 - mean_q: 1.089 - mean_eps: 0.100 - ale.lives: 2.196\n",
            "\n",
            "Interval 187 (1860000 steps performed)\n",
            "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0226\n",
            "12 episodes - episode_reward: 20.083 [9.000, 28.000] - loss: 0.010 - mae: 0.903 - mean_q: 1.087 - mean_eps: 0.100 - ale.lives: 2.184\n",
            "\n",
            "Interval 188 (1870000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0230\n",
            "11 episodes - episode_reward: 19.727 [14.000, 28.000] - loss: 0.010 - mae: 0.904 - mean_q: 1.087 - mean_eps: 0.100 - ale.lives: 2.141\n",
            "\n",
            "Interval 189 (1880000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0241\n",
            "12 episodes - episode_reward: 20.167 [13.000, 31.000] - loss: 0.010 - mae: 0.896 - mean_q: 1.078 - mean_eps: 0.100 - ale.lives: 2.159\n",
            "\n",
            "Interval 190 (1890000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0227\n",
            "12 episodes - episode_reward: 19.833 [11.000, 32.000] - loss: 0.010 - mae: 0.884 - mean_q: 1.063 - mean_eps: 0.100 - ale.lives: 2.204\n",
            "\n",
            "Interval 191 (1900000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0212\n",
            "11 episodes - episode_reward: 18.818 [7.000, 29.000] - loss: 0.010 - mae: 0.882 - mean_q: 1.062 - mean_eps: 0.100 - ale.lives: 2.111\n",
            "\n",
            "Interval 192 (1910000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0205\n",
            "12 episodes - episode_reward: 16.833 [6.000, 29.000] - loss: 0.010 - mae: 0.885 - mean_q: 1.065 - mean_eps: 0.100 - ale.lives: 2.084\n",
            "\n",
            "Interval 193 (1920000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0234\n",
            "12 episodes - episode_reward: 19.917 [9.000, 34.000] - loss: 0.010 - mae: 0.891 - mean_q: 1.072 - mean_eps: 0.100 - ale.lives: 2.142\n",
            "\n",
            "Interval 194 (1930000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0256\n",
            "10 episodes - episode_reward: 24.200 [14.000, 35.000] - loss: 0.010 - mae: 0.893 - mean_q: 1.075 - mean_eps: 0.100 - ale.lives: 2.378\n",
            "\n",
            "Interval 195 (1940000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0213\n",
            "13 episodes - episode_reward: 17.154 [4.000, 35.000] - loss: 0.010 - mae: 0.897 - mean_q: 1.079 - mean_eps: 0.100 - ale.lives: 2.359\n",
            "\n",
            "Interval 196 (1950000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0234\n",
            "12 episodes - episode_reward: 20.167 [9.000, 33.000] - loss: 0.010 - mae: 0.906 - mean_q: 1.090 - mean_eps: 0.100 - ale.lives: 2.085\n",
            "\n",
            "Interval 197 (1960000 steps performed)\n",
            "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0234\n",
            "11 episodes - episode_reward: 20.455 [11.000, 28.000] - loss: 0.010 - mae: 0.896 - mean_q: 1.078 - mean_eps: 0.100 - ale.lives: 2.223\n",
            "\n",
            "Interval 198 (1970000 steps performed)\n",
            "10000/10000 [==============================] - 408s 41ms/step - reward: 0.0235\n",
            "11 episodes - episode_reward: 20.727 [9.000, 27.000] - loss: 0.011 - mae: 0.894 - mean_q: 1.074 - mean_eps: 0.100 - ale.lives: 2.030\n",
            "\n",
            "Interval 199 (1980000 steps performed)\n",
            "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0226\n",
            "12 episodes - episode_reward: 19.583 [8.000, 29.000] - loss: 0.011 - mae: 0.897 - mean_q: 1.079 - mean_eps: 0.100 - ale.lives: 2.113\n",
            "\n",
            "Interval 200 (1990000 steps performed)\n",
            "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0222\n",
            "done, took 68774.686 seconds\n"
          ]
        }
      ],
      "source": [
        "# Training part\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "\n",
        "dqn.fit(env, callbacks=callbacks, nb_steps=2000000, log_interval=10000, visualize=False)\n",
        "\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHYryKd1Gb2b",
        "outputId": "c707c962-8015-4a4a-abf1-f09f6ee2a007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 528\n",
            "Episode 2: reward: 34.000, steps: 1320\n",
            "Episode 3: reward: 30.000, steps: 1067\n",
            "Episode 4: reward: 13.000, steps: 641\n",
            "Episode 5: reward: 18.000, steps: 775\n",
            "Episode 6: reward: 17.000, steps: 857\n",
            "Episode 7: reward: 19.000, steps: 928\n",
            "Episode 8: reward: 16.000, steps: 707\n",
            "Episode 9: reward: 13.000, steps: 642\n",
            "Episode 10: reward: 28.000, steps: 1109\n",
            "Episode 11: reward: 29.000, steps: 1181\n",
            "Episode 12: reward: 22.000, steps: 1016\n",
            "Episode 13: reward: 32.000, steps: 1036\n",
            "Episode 14: reward: 19.000, steps: 784\n",
            "Episode 15: reward: 17.000, steps: 621\n",
            "Episode 16: reward: 32.000, steps: 1453\n",
            "Episode 17: reward: 15.000, steps: 666\n",
            "Episode 18: reward: 28.000, steps: 1140\n",
            "Episode 19: reward: 16.000, steps: 724\n",
            "Episode 20: reward: 28.000, steps: 871\n",
            "Episode 21: reward: 12.000, steps: 631\n",
            "Episode 22: reward: 13.000, steps: 514\n",
            "Episode 23: reward: 22.000, steps: 906\n",
            "Episode 24: reward: 20.000, steps: 924\n",
            "Episode 25: reward: 20.000, steps: 961\n",
            "Episode 26: reward: 21.000, steps: 764\n",
            "Episode 27: reward: 16.000, steps: 778\n",
            "Episode 28: reward: 14.000, steps: 643\n",
            "Episode 29: reward: 16.000, steps: 811\n",
            "Episode 30: reward: 14.000, steps: 756\n",
            "Episode 31: reward: 19.000, steps: 671\n",
            "Episode 32: reward: 15.000, steps: 701\n",
            "Episode 33: reward: 14.000, steps: 722\n",
            "Episode 34: reward: 19.000, steps: 892\n",
            "Episode 35: reward: 18.000, steps: 780\n",
            "Episode 36: reward: 17.000, steps: 649\n",
            "Episode 37: reward: 17.000, steps: 1005\n",
            "Episode 38: reward: 29.000, steps: 1072\n",
            "Episode 39: reward: 23.000, steps: 867\n",
            "Episode 40: reward: 14.000, steps: 706\n",
            "Episode 41: reward: 30.000, steps: 1290\n",
            "Episode 42: reward: 27.000, steps: 1200\n",
            "Episode 43: reward: 12.000, steps: 693\n",
            "Episode 44: reward: 13.000, steps: 769\n",
            "Episode 45: reward: 33.000, steps: 1398\n",
            "Episode 46: reward: 31.000, steps: 1391\n",
            "Episode 47: reward: 12.000, steps: 618\n",
            "Episode 48: reward: 25.000, steps: 1034\n",
            "Episode 49: reward: 28.000, steps: 1031\n",
            "Episode 50: reward: 15.000, steps: 694\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x16831b7a880>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=50, visualize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAlu8b1Gb2b"
      },
      "source": [
        "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahEbP4HHXCFb"
      },
      "source": [
        "**Modelo 1:**\n",
        "\n",
        "Red neuronal: 3 capas convolucionales con un kernel de 3x3 en cada capa y stride diferenciado. Total de parámetros a entrenar = 2.157.350\n",
        "\n",
        "Policy: DQN utilizando el algoritmo MaxBoltzmannQPolicy.\n",
        "\n",
        "Steps de entrenamiento: **1.930.000**\n",
        "\n",
        "Resultados en test (promedio): **13,6**\n",
        "\n",
        "Conclusiones: El agente tiene un comportamiento muy aleatorio y puede llegar en entrenamiento a obtener recompensas por encima de los 30 puntos pero también puede llegar a un mínimo de 2 puntos. La mejora en la red neuronal con respecto al ejemplo de clase no hizo una gran diferencia y se entiende que se debe al uso del algortimo MaxBoltzmannQPolicy la cual es una combinación de eps-greedy (acción aleatoria basada en epsilon) y Boltzman q-policy (Probabilidad basada en q).\n",
        "\n",
        "Anexo 1: Imágenes de la red neuronal, resultados en entrenamiento y test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_BxBptnXCFb"
      },
      "source": [
        "\n",
        "**Modelo 2:**\n",
        "\n",
        "Red neuronal: 3 capas convolucionales, con los siguientes detalles:\n",
        "- Primera Capa Convolucional:\n",
        "Kernel: 8x8.\n",
        "Strides: 4X4\n",
        "\n",
        "- Segunda Capa Convolucional:\n",
        "Kernel: 4x4.\n",
        "Strides: 2X2\n",
        "\n",
        "- Tercera Capa Convolucional:\n",
        "Kernel: 3x3.\n",
        "Strides: 1X1\n",
        "\n",
        "\n",
        "Total de parámetros a entrenar = 34,812,326\n",
        "\n",
        "Policy: DQN utilizando el algoritmo EpsGreedyQPolicy.\n",
        "\n",
        "Steps de entrenamiento: **5.000**\n",
        "\n",
        "Resultados en test (promedio): **18.97**\n",
        "\n",
        "*Conclusiones:*\n",
        "- El agente tiene un rendimiento variado con recompensas que oscilan entre 5 y 30. Esto indica cierta eficacia en el aprendizaje, pero con espacio para mejorar la consistencia.\n",
        "- El rendimiento promedio está cerca del objetivo de 20, lo cual es positivo, pero la meta es alcanzar o superar consistentemente este puntaje.\n",
        "- La policy de exploración actual parece adecuada, pero se podría experimentar con los parámetros de LinearAnnealedPolicy y EpsGreedyQPolicy para optimizar aún más el equilibrio entre exploración y explotación.\n",
        "- La arquitectura de la red es apropiada, aunque siempre hay espacio para experimentación.\n",
        "- Como objetivo de equipo, se plantea realizar ajustes en la arquitectura del modelo y en la estrategia de entrenamiento para lograr un rendimiento más consistente y fiable que alcance de manera regular nuestro objetivo de puntaje.\n",
        "\n",
        "Anexo 2: Imágenes de la red neuronal, resultados en entrenamiento y test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47_Tdwo6XCFb"
      },
      "source": [
        "**Modelo 3:**\n",
        "\n",
        "Red neuronal: 3 capas convolucionales con un kernel de 3x3 en cada capa y stride diferenciado. Total de parámetros a entrenar = 508,006\n",
        "\n",
        "Policy: DQN utilizando el algoritmo MaxBoltzmannQPolicy.\n",
        "\n",
        "Steps de entrenamiento: **1.750.000**\n",
        "\n",
        "Resultados en test (promedio): **+20**\n",
        "\n",
        "Conclusiones: Se trata del modelo realizado en la clase, sin ninguna modificación y que se realizó en primera instancia para hacer pruebas. Finalmente parece ser el que mejor resultado ha dado, superando la recompensa de 20 de media. Aún así, al no presentar ninguna característica original se ha optado por entregar el modelo que presentamos a continuación.\n",
        "\n",
        "Anexo 3: Imágenes de la red neuronal, resultados en entrenamiento y test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdXWd__pXCFb"
      },
      "source": [
        "**Modelo Final**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Red neuronal: 3 capas convolucionales, con los siguientes detalles:\n",
        "\n",
        "- Primera Capa Convolucional: Kernel: 3 x 3. Strides: 3X3\n",
        "\n",
        "- Segunda Capa Convolucional: Kernel: 3 x 3. Strides: 1X1\n",
        "\n",
        "- Tercera Capa Convolucional: Kernel: 3x3. Strides: 1X1\n",
        "\n",
        "Dos capas densas de 512 perceptrones.\n",
        "\n",
        "Total de parámetros a entrenar = 949,606\n",
        "\n",
        "Policy: DQN utilizando el algoritmo EpsGreedyQPolicy modificado.\n",
        "\n",
        "La modificación ha consistido en hacer que el epsilon descienda poco a poco. Por este motivo no se ha fijado el parámetro *nb_steps_warmup* en la función de entrenamiento, puesto que la exploración se ha hecho progresivamente según se avanzaba en el entrenamiento.\n",
        "\n",
        "Para el preprocesamiento de las observaciones se ha tratado de hacer un crop de esta para enviar a la red neuronal solo la información del juego, eliminando el score y el suelo de abajo.\n",
        "\n",
        "Steps de entrenamiento: **2.000.000**\n",
        "\n",
        "Resultados en test (promedio): **20.30**\n",
        "\n",
        "*Conclusiones:*\n",
        "- El agente tiene un rendimiento variado con recompensas que oscilan entre 11 y 34. Esto indica cierta eficacia en el aprendizaje, pero con espacio para mejorar la consistencia. Otros experimentos realizados por el grupo utilizando una red neuronal con más parámetros han presentado resultados parejos con menos steps.\n",
        "- El rendimiento promedio está cerca del objetivo de 20, lo cual es positivo, pero la meta es alcanzar o superar consistentemente este puntaje.\n",
        "- La policy de exploración actual parece adecuada, pero la mejora sobre el valor del epsilon no ha presentado el rendimiento que se esperaba, quedando un promedio de recompensa por debajo del objetivo.\n",
        "- En la arquitectura de la red se ha optado por una red simple, sin un número elevado de parámetros para que el rendimiento fuera mejor. Esto puede haber sido un error, ya que como se ha comentado arriba, otros experimentos con RN más grandes hhan presentado un resultado similar.- Como objetivo de equipo, se plantea realizar ajustes en la arquitectura del modelo y en la estrategia de entrenamiento para lograr un rendimiento más consistente y fiable que alcance de manera regular nuestro objetivo de puntaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wea3lgqmXCFb"
      },
      "source": [
        "*Conclusiones finales*\n",
        "\n",
        "Recopilando todos los experimentos realizados, podemos apreciar, aunque no afirmar rotundamente los siguentes puntos.\n",
        "\n",
        "    - Una red neuronal pesada puede dar mejores resultados con menos steps, lo que podría compensar el coste en tiempo del Backpropagation de la red.\n",
        "    - Hacer un crop de la observación y reducir el epsilon progresivamente no han supuesto una mejora notable, por lo que en un nuevo intento se podrian evitar estos pasos y trabajar de una manera más estandard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFQiicXK3sO"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}