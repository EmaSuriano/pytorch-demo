{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (3135209932.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 77\u001b[0;36m\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from model import DQN\n",
    "from torch import optim\n",
    "from utils import Transition, ReplayMemory, VideoRecorder\n",
    "from wrapper import AtariWrapper\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import deque\n",
    "import ale_py\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "\n",
    "# some hyperparameters\n",
    "GAMMA = 0.99  # bellman function\n",
    "EPS_START = 1.0  # Start with full exploration\n",
    "EPS_END = 0.05  # Minimum exploration rate\n",
    "EPS_DECAY = 50000  # Slower decay\n",
    "WARMUP = 5000  # don't update net until WARMUP steps\n",
    "TARGET_UPDATE = 10000\n",
    "LR = 1e-5\n",
    "BATCH_SIZE = 128\n",
    "N_EPISODES = 2000\n",
    "EVAL_CYCLE = N_EPISODES / 10\n",
    "\n",
    "steps_done = 0\n",
    "eps_threshold = EPS_START\n",
    "\n",
    "\n",
    "def select_action(state: torch.Tensor) -> torch.Tensor:\n",
    "    global eps_threshold\n",
    "    global steps_done\n",
    "\n",
    "    # Exponential decay of exploration rate\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(\n",
    "        -1.0 * steps_done / EPS_DECAY\n",
    "    )\n",
    "\n",
    "    # Ensure eps_threshold doesn't get stuck\n",
    "    eps_threshold = max(eps_threshold, EPS_END)\n",
    "\n",
    "    sample = random.random()\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]]).to(device)\n",
    "\n",
    "\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "env = AtariWrapper(env)\n",
    "\n",
    "# n_action = 6\n",
    "n_action = env.action_space.n\n",
    "\n",
    "log_dir = os.path.join(\"logs\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "log_path = os.path.join(log_dir, \"log.txt\")\n",
    "\n",
    "# tensorboard\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\n",
    "# video\n",
    "video = VideoRecorder(log_dir)\n",
    "\n",
    "# create network and target network\n",
    "policy_net = DQN(in_channels=4, n_actions=n_action).to(device)\n",
    "target_net = DQN(in_channels=4, n_actions=n_action).to(device)\n",
    "\n",
    "# let target model = model\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# replay memory\n",
    "memory = ReplayMemory(50000)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# warming up\n",
    "print(\"Warming up...\")\n",
    "warmupstep = 0\n",
    "for _ in range(WARMUP):\n",
    "    obs, info = env.reset()  # (84,84)\n",
    "    obs = torch.from_numpy(obs).to(device)  # (84,84)\n",
    "    # stack four frames together, hoping to learn temporal info\n",
    "    obs = torch.stack((obs, obs, obs, obs)).unsqueeze(0)  # (1,4,84,84)\n",
    "    done = False\n",
    "\n",
    "    # step loop\n",
    "    while not done:\n",
    "        warmupstep += 1\n",
    "        # take one step\n",
    "        action = torch.tensor([[env.action_space.sample()]]).to(device)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # convert to tensor\n",
    "        reward = torch.tensor([reward], device=device)  # (1)\n",
    "        done = torch.tensor([done], device=device)  # (1)\n",
    "        next_obs = torch.from_numpy(next_obs).to(device)  # (84,84)\n",
    "        next_obs = torch.stack((next_obs, obs[0][0], obs[0][1], obs[0][2])).unsqueeze(\n",
    "            0\n",
    "        )  # (1,4,84,84)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(obs, action, next_obs, reward, done)\n",
    "\n",
    "        # move to next state\n",
    "        obs = next_obs\n",
    "\n",
    "rewardList = []\n",
    "lossList = []\n",
    "rewarddeq = deque([], maxlen=100)\n",
    "lossdeq = deque([], maxlen=100)\n",
    "avgrewardlist = []\n",
    "avglosslist = []\n",
    "\n",
    "\n",
    "def eval_model():\n",
    "    video.reset()\n",
    "    evalenv = gym.make(\"SpaceInvadersNoFrameskip-v4\")\n",
    "    evalenv = AtariWrapper(evalenv, video=video)\n",
    "    obs, info = evalenv.reset()\n",
    "    obs = torch.from_numpy(obs).to(device)\n",
    "    obs = torch.stack((obs, obs, obs, obs)).unsqueeze(0)\n",
    "    evalreward = 0\n",
    "    policy_net.eval()\n",
    "\n",
    "    for _ in count():\n",
    "        action = policy_net(obs).max(1)[1]\n",
    "        next_obs, reward, terminated, truncated, info = evalenv.step(action.item())\n",
    "        evalreward += reward\n",
    "        next_obs = torch.from_numpy(next_obs).to(device)  # (84,84)\n",
    "        next_obs = torch.stack((next_obs, obs[0][0], obs[0][1], obs[0][2])).unsqueeze(\n",
    "            0\n",
    "        )  # (1,4,84,84)\n",
    "        obs = next_obs\n",
    "        if terminated or truncated:\n",
    "            if info[\"lives\"] == 0:  # real end\n",
    "                break\n",
    "            else:\n",
    "                obs, info = evalenv.reset()\n",
    "                obs = torch.from_numpy(obs).to(device)\n",
    "                obs = torch.stack((obs, obs, obs, obs)).unsqueeze(0)\n",
    "\n",
    "    evalenv.close()\n",
    "    video.save(f\"{epoch}.mp4\")\n",
    "    torch.save(policy_net, os.path.join(log_dir, f\"model{epoch}.pth\"))\n",
    "    print(f\"Eval epoch {epoch}: Reward {evalreward}\")\n",
    "\n",
    "\n",
    "# epoch loop\n",
    "for epoch in range(N_EPISODES + 1):\n",
    "    obs, info = env.reset()  # (84,84)\n",
    "    obs = torch.from_numpy(obs).to(device)  # (84,84)\n",
    "    # stack four frames together, hoping to learn temporal info\n",
    "    obs = torch.stack((obs, obs, obs, obs)).unsqueeze(0)  # (1,4,84,84)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_reward = 0\n",
    "\n",
    "    # step loop\n",
    "    for step in count():\n",
    "        # take one step\n",
    "        action = select_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # convert to tensor\n",
    "        reward = torch.tensor([reward], device=device)  # (1)\n",
    "        done = torch.tensor([done], device=device)  # (1)\n",
    "        next_obs = torch.from_numpy(next_obs).to(device)  # (84,84)\n",
    "        next_obs = torch.stack((next_obs, obs[0][0], obs[0][1], obs[0][2])).unsqueeze(\n",
    "            0\n",
    "        )  # (1,4,84,84)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(obs, action, next_obs, reward, done)\n",
    "\n",
    "        # move to next state\n",
    "        obs = next_obs\n",
    "\n",
    "        # train\n",
    "        policy_net.train()\n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(\n",
    "            *zip(*transitions)\n",
    "        )  # batch-array of Transitions -> Transition of batch-arrays.\n",
    "        state_batch = torch.cat(batch.state)  # (bs,4,84,84)\n",
    "        next_state_batch = torch.cat(batch.next_state)  # (bs,4,84,84)\n",
    "        action_batch = torch.cat(batch.action)  # (bs,1)\n",
    "        reward_batch = torch.cat(batch.reward).unsqueeze(1)  # (bs,1)\n",
    "        done_batch = torch.cat(batch.done).unsqueeze(1)  # (bs,1)\n",
    "\n",
    "        # Q(st,a)\n",
    "        state_qvalues = policy_net(state_batch)  # (bs,n_actions)\n",
    "        selected_state_qvalue = state_qvalues.gather(1, action_batch)  # (bs,1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Q'(st+1,a)\n",
    "            next_state_target_qvalues = target_net(next_state_batch)  # (bs,n_actions)\n",
    "            next_state_selected_qvalue = next_state_target_qvalues.max(1, keepdim=True)[\n",
    "                0\n",
    "            ]  # (bs,1)\n",
    "\n",
    "        # td target\n",
    "        tdtarget = (\n",
    "            next_state_selected_qvalue * GAMMA * ~done_batch + reward_batch\n",
    "        )  # (bs,1)\n",
    "\n",
    "        # optimize\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(selected_state_qvalue, tdtarget)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # let target_net = policy_net every 1000 steps\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            # eval\n",
    "            if epoch % EVAL_CYCLE == 0:\n",
    "                with torch.no_grad():\n",
    "                    eval_model()\n",
    "            break\n",
    "\n",
    "    rewardList.append(total_reward)\n",
    "    lossList.append(total_loss)\n",
    "    rewarddeq.append(total_reward)\n",
    "    lossdeq.append(total_loss)\n",
    "    avgreward = sum(rewarddeq) / len(rewarddeq)\n",
    "    avgloss = sum(lossdeq) / len(lossdeq)\n",
    "    avglosslist.append(avgloss)\n",
    "    avgrewardlist.append(avgreward)\n",
    "\n",
    "    writer.add_scalar(\"Reward/Total\", total_reward, epoch)\n",
    "    writer.add_scalar(\"Loss/Total\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Metrics/Average Reward\", avgreward, epoch)\n",
    "    writer.add_scalar(\"Metrics/Average Loss\", avgloss, epoch)\n",
    "    writer.add_scalar(\"Hyperparameters/Epsilon\", eps_threshold, epoch)\n",
    "    writer.add_scalar(\"Metrics/Steps Done\", steps_done, epoch)\n",
    "\n",
    "    output = f\"Epoch {epoch}: Loss {total_loss:.2f}, Reward {total_reward}, Avgloss {avgloss:.2f}, Avgreward {avgreward:.2f}, Epsilon {eps_threshold:.2f}, TotalStep {steps_done}\"\n",
    "    print(output)\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(f\"{output}\\n\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "# plot loss-epoch and reward-epoch\n",
    "plt.figure(1)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(range(len(lossList)), lossList, label=\"loss\")\n",
    "plt.plot(range(len(lossList)), avglosslist, label=\"avg\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(log_dir, \"loss.png\"))\n",
    "\n",
    "plt.figure(2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(range(len(rewardList)), rewardList, label=\"reward\")\n",
    "plt.plot(range(len(rewardList)), avgrewardlist, label=\"avg\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(log_dir, \"reward.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
